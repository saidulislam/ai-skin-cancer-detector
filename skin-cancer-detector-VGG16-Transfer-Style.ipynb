{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All the imports\n",
    "\n",
    "%matplotlib inline\n",
    "# python libraties\n",
    "import os, cv2,itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim,nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models,transforms,datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders and transformsÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  2000\n",
      "Valid data size:  150\n",
      "Test data size:  600\n"
     ]
    }
   ],
   "source": [
    "## data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "\n",
    "## AlERT !!!! We are going to training our data with Inception model and this model take 299x299 images\n",
    "train_transform = transforms.Compose([transforms.RandomRotation(10),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomVerticalFlip(),\n",
    "                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n",
    "                                      transforms.Resize(299),\n",
    "                                      transforms.CenterCrop(299),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transform = transforms.Compose([transforms.Resize(299),\n",
    "                                      transforms.CenterCrop(299),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(299),\n",
    "                                      transforms.CenterCrop(299),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "root_dir = 'data'\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root_dir, 'train'), transform=train_transform)\n",
    "valid_data = datasets.ImageFolder(os.path.join(root_dir, 'valid'), transform=valid_transform)\n",
    "test_data = datasets.ImageFolder(os.path.join(root_dir, 'test'), transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=10, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)\n",
    "\n",
    "print('Training data size: ',len(train_data))\n",
    "print('Valid data size: ',len(valid_data))\n",
    "print('Test data size: ',len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Specify model architecture \n",
    "model_transfer = models.vgg16(pretrained=True)\n",
    "\n",
    "# freeze parameters of the model\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# update the classifier part of model_transfer\n",
    "classifier = nn.Sequential(nn.Linear(25088, 1024),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(0.5),\n",
    "                           nn.Linear(1024, 3))\n",
    "model_transfer.classifier = classifier\n",
    "\n",
    "# check if cuda is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cpu')\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()\n",
    "    device = torch.device('cuda:0')\n",
    "    \n",
    "print(model_transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model_transfer.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \tBatch:20 \tTrain Loss: 1.0278104543685913\n",
      "\n",
      "Epoch:1 \tBatch:40 \tTrain Loss: 0.9111490249633789\n",
      "\n",
      "Epoch:1 \tBatch:60 \tTrain Loss: 0.8659690618515015\n",
      "\n",
      "Epoch:1 \tBatch:80 \tTrain Loss: 0.7995301485061646\n",
      "\n",
      "Epoch:1 \tBatch:100 \tTrain Loss: 0.7972770929336548\n",
      "\n",
      "Epoch:1 \tBatch:120 \tTrain Loss: 0.8148987293243408\n",
      "\n",
      "Epoch:1 \tBatch:140 \tTrain Loss: 0.8123986721038818\n",
      "\n",
      "Epoch:1 \tBatch:160 \tTrain Loss: 0.8140407204627991\n",
      "\n",
      "Epoch:1 \tBatch:180 \tTrain Loss: 0.8061633706092834\n",
      "\n",
      "Epoch:1 \tBatch:200 \tTrain Loss: 0.7990249395370483\n",
      "\n",
      "Epoch: 1 \tTraining Loss: 0.799025 \tValidation Loss: 0.776437\n",
      "Validation loss decreased (inf --> 0.776437). Saving model...\n",
      "Epoch:2 \tBatch:20 \tTrain Loss: 0.6472934484481812\n",
      "\n",
      "Epoch:2 \tBatch:40 \tTrain Loss: 0.6642495393753052\n",
      "\n",
      "Epoch:2 \tBatch:60 \tTrain Loss: 0.674979567527771\n",
      "\n",
      "Epoch:2 \tBatch:80 \tTrain Loss: 0.700082004070282\n",
      "\n",
      "Epoch:2 \tBatch:100 \tTrain Loss: 0.6957595944404602\n",
      "\n",
      "Epoch:2 \tBatch:120 \tTrain Loss: 0.7068817019462585\n",
      "\n",
      "Epoch:2 \tBatch:140 \tTrain Loss: 0.7041639089584351\n",
      "\n",
      "Epoch:2 \tBatch:160 \tTrain Loss: 0.6983562111854553\n",
      "\n",
      "Epoch:2 \tBatch:180 \tTrain Loss: 0.6971766948699951\n",
      "\n",
      "Epoch:2 \tBatch:200 \tTrain Loss: 0.6928123235702515\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 0.692812 \tValidation Loss: 0.810739\n",
      "Epoch:3 \tBatch:20 \tTrain Loss: 0.5954089164733887\n",
      "\n",
      "Epoch:3 \tBatch:40 \tTrain Loss: 0.6372351050376892\n",
      "\n",
      "Epoch:3 \tBatch:60 \tTrain Loss: 0.6422321796417236\n",
      "\n",
      "Epoch:3 \tBatch:80 \tTrain Loss: 0.6229020953178406\n",
      "\n",
      "Epoch:3 \tBatch:100 \tTrain Loss: 0.6200186014175415\n",
      "\n",
      "Epoch:3 \tBatch:120 \tTrain Loss: 0.6180145740509033\n",
      "\n",
      "Epoch:3 \tBatch:140 \tTrain Loss: 0.6280826926231384\n",
      "\n",
      "Epoch:3 \tBatch:160 \tTrain Loss: 0.6353376507759094\n",
      "\n",
      "Epoch:3 \tBatch:180 \tTrain Loss: 0.6385469436645508\n",
      "\n",
      "Epoch:3 \tBatch:200 \tTrain Loss: 0.6374519467353821\n",
      "\n",
      "Epoch: 3 \tTraining Loss: 0.637452 \tValidation Loss: 0.796695\n",
      "Epoch:4 \tBatch:20 \tTrain Loss: 0.6338530778884888\n",
      "\n",
      "Epoch:4 \tBatch:40 \tTrain Loss: 0.590036928653717\n",
      "\n",
      "Epoch:4 \tBatch:60 \tTrain Loss: 0.5718498229980469\n",
      "\n",
      "Epoch:4 \tBatch:80 \tTrain Loss: 0.5718889236450195\n",
      "\n",
      "Epoch:4 \tBatch:100 \tTrain Loss: 0.5694749355316162\n",
      "\n",
      "Epoch:4 \tBatch:120 \tTrain Loss: 0.5806707143783569\n",
      "\n",
      "Epoch:4 \tBatch:140 \tTrain Loss: 0.5895366072654724\n",
      "\n",
      "Epoch:4 \tBatch:160 \tTrain Loss: 0.591747522354126\n",
      "\n",
      "Epoch:4 \tBatch:180 \tTrain Loss: 0.5989595055580139\n",
      "\n",
      "Epoch:4 \tBatch:200 \tTrain Loss: 0.6086399555206299\n",
      "\n",
      "Epoch: 4 \tTraining Loss: 0.608640 \tValidation Loss: 0.802999\n",
      "Epoch:5 \tBatch:20 \tTrain Loss: 0.6055657863616943\n",
      "\n",
      "Epoch:5 \tBatch:40 \tTrain Loss: 0.6037003397941589\n",
      "\n",
      "Epoch:5 \tBatch:60 \tTrain Loss: 0.6005363464355469\n",
      "\n",
      "Epoch:5 \tBatch:80 \tTrain Loss: 0.5983965992927551\n",
      "\n",
      "Epoch:5 \tBatch:100 \tTrain Loss: 0.5914201736450195\n",
      "\n",
      "Epoch:5 \tBatch:120 \tTrain Loss: 0.5921311974525452\n",
      "\n",
      "Epoch:5 \tBatch:140 \tTrain Loss: 0.5958529114723206\n",
      "\n",
      "Epoch:5 \tBatch:160 \tTrain Loss: 0.5915402173995972\n",
      "\n",
      "Epoch:5 \tBatch:180 \tTrain Loss: 0.5882861018180847\n",
      "\n",
      "Epoch:5 \tBatch:200 \tTrain Loss: 0.5888106226921082\n",
      "\n",
      "Epoch: 5 \tTraining Loss: 0.588811 \tValidation Loss: 0.740060\n",
      "Validation loss decreased (0.776437 --> 0.740060). Saving model...\n",
      "Epoch:6 \tBatch:20 \tTrain Loss: 0.6495833992958069\n",
      "\n",
      "Epoch:6 \tBatch:40 \tTrain Loss: 0.6646851897239685\n",
      "\n",
      "Epoch:6 \tBatch:60 \tTrain Loss: 0.6126741170883179\n",
      "\n",
      "Epoch:6 \tBatch:80 \tTrain Loss: 0.6058736443519592\n",
      "\n",
      "Epoch:6 \tBatch:100 \tTrain Loss: 0.5828336477279663\n",
      "\n",
      "Epoch:6 \tBatch:120 \tTrain Loss: 0.5775154232978821\n",
      "\n",
      "Epoch:6 \tBatch:140 \tTrain Loss: 0.5846632122993469\n",
      "\n",
      "Epoch:6 \tBatch:160 \tTrain Loss: 0.5794495940208435\n",
      "\n",
      "Epoch:6 \tBatch:180 \tTrain Loss: 0.5708028674125671\n",
      "\n",
      "Epoch:6 \tBatch:200 \tTrain Loss: 0.5755547285079956\n",
      "\n",
      "Epoch: 6 \tTraining Loss: 0.575555 \tValidation Loss: 0.687650\n",
      "Validation loss decreased (0.740060 --> 0.687650). Saving model...\n",
      "Epoch:7 \tBatch:20 \tTrain Loss: 0.510689914226532\n",
      "\n",
      "Epoch:7 \tBatch:40 \tTrain Loss: 0.4796009957790375\n",
      "\n",
      "Epoch:7 \tBatch:60 \tTrain Loss: 0.492607057094574\n",
      "\n",
      "Epoch:7 \tBatch:80 \tTrain Loss: 0.4973360002040863\n",
      "\n",
      "Epoch:7 \tBatch:100 \tTrain Loss: 0.5041646361351013\n",
      "\n",
      "Epoch:7 \tBatch:120 \tTrain Loss: 0.5308430790901184\n",
      "\n",
      "Epoch:7 \tBatch:140 \tTrain Loss: 0.5398440361022949\n",
      "\n",
      "Epoch:7 \tBatch:160 \tTrain Loss: 0.5426046848297119\n",
      "\n",
      "Epoch:7 \tBatch:180 \tTrain Loss: 0.5415968894958496\n",
      "\n",
      "Epoch:7 \tBatch:200 \tTrain Loss: 0.5513866543769836\n",
      "\n",
      "Epoch: 7 \tTraining Loss: 0.551387 \tValidation Loss: 0.747320\n",
      "Epoch:8 \tBatch:20 \tTrain Loss: 0.5437381267547607\n",
      "\n",
      "Epoch:8 \tBatch:40 \tTrain Loss: 0.5291262865066528\n",
      "\n",
      "Epoch:8 \tBatch:60 \tTrain Loss: 0.5089205503463745\n",
      "\n",
      "Epoch:8 \tBatch:80 \tTrain Loss: 0.5299943685531616\n",
      "\n",
      "Epoch:8 \tBatch:100 \tTrain Loss: 0.5270360112190247\n",
      "\n",
      "Epoch:8 \tBatch:120 \tTrain Loss: 0.5271086692810059\n",
      "\n",
      "Epoch:8 \tBatch:140 \tTrain Loss: 0.5326172113418579\n",
      "\n",
      "Epoch:8 \tBatch:160 \tTrain Loss: 0.5302047729492188\n",
      "\n",
      "Epoch:8 \tBatch:180 \tTrain Loss: 0.5232973098754883\n",
      "\n",
      "Epoch:8 \tBatch:200 \tTrain Loss: 0.527774453163147\n",
      "\n",
      "Epoch: 8 \tTraining Loss: 0.527774 \tValidation Loss: 0.709234\n",
      "Epoch:9 \tBatch:20 \tTrain Loss: 0.5484528541564941\n",
      "\n",
      "Epoch:9 \tBatch:40 \tTrain Loss: 0.544602632522583\n",
      "\n",
      "Epoch:9 \tBatch:60 \tTrain Loss: 0.5414819121360779\n",
      "\n",
      "Epoch:9 \tBatch:80 \tTrain Loss: 0.5324501395225525\n",
      "\n",
      "Epoch:9 \tBatch:100 \tTrain Loss: 0.527941107749939\n",
      "\n",
      "Epoch:9 \tBatch:120 \tTrain Loss: 0.5208771228790283\n",
      "\n",
      "Epoch:9 \tBatch:140 \tTrain Loss: 0.5377662181854248\n",
      "\n",
      "Epoch:9 \tBatch:160 \tTrain Loss: 0.5341261625289917\n",
      "\n",
      "Epoch:9 \tBatch:180 \tTrain Loss: 0.5375775694847107\n",
      "\n",
      "Epoch:9 \tBatch:200 \tTrain Loss: 0.536663830280304\n",
      "\n",
      "Epoch: 9 \tTraining Loss: 0.536664 \tValidation Loss: 0.696594\n",
      "Epoch:10 \tBatch:20 \tTrain Loss: 0.552932858467102\n",
      "\n",
      "Epoch:10 \tBatch:40 \tTrain Loss: 0.4868534505367279\n",
      "\n",
      "Epoch:10 \tBatch:60 \tTrain Loss: 0.47502830624580383\n",
      "\n",
      "Epoch:10 \tBatch:80 \tTrain Loss: 0.48381558060646057\n",
      "\n",
      "Epoch:10 \tBatch:100 \tTrain Loss: 0.4963526129722595\n",
      "\n",
      "Epoch:10 \tBatch:120 \tTrain Loss: 0.48955070972442627\n",
      "\n",
      "Epoch:10 \tBatch:140 \tTrain Loss: 0.49008145928382874\n",
      "\n",
      "Epoch:10 \tBatch:160 \tTrain Loss: 0.49148663878440857\n",
      "\n",
      "Epoch:10 \tBatch:180 \tTrain Loss: 0.5002774000167847\n",
      "\n",
      "Epoch:10 \tBatch:200 \tTrain Loss: 0.49920275807380676\n",
      "\n",
      "Epoch: 10 \tTraining Loss: 0.499203 \tValidation Loss: 0.687752\n",
      "Epoch:11 \tBatch:20 \tTrain Loss: 0.43901345133781433\n",
      "\n",
      "Epoch:11 \tBatch:40 \tTrain Loss: 0.46147623658180237\n",
      "\n",
      "Epoch:11 \tBatch:60 \tTrain Loss: 0.45306873321533203\n",
      "\n",
      "Epoch:11 \tBatch:80 \tTrain Loss: 0.4743785560131073\n",
      "\n",
      "Epoch:11 \tBatch:100 \tTrain Loss: 0.4802343547344208\n",
      "\n",
      "Epoch:11 \tBatch:120 \tTrain Loss: 0.48419952392578125\n",
      "\n",
      "Epoch:11 \tBatch:140 \tTrain Loss: 0.48851490020751953\n",
      "\n",
      "Epoch:11 \tBatch:160 \tTrain Loss: 0.4912858307361603\n",
      "\n",
      "Epoch:11 \tBatch:180 \tTrain Loss: 0.49507465958595276\n",
      "\n",
      "Epoch:11 \tBatch:200 \tTrain Loss: 0.49860623478889465\n",
      "\n",
      "Epoch: 11 \tTraining Loss: 0.498606 \tValidation Loss: 0.688286\n",
      "Epoch:12 \tBatch:20 \tTrain Loss: 0.4352665841579437\n",
      "\n",
      "Epoch:12 \tBatch:40 \tTrain Loss: 0.4103940725326538\n",
      "\n",
      "Epoch:12 \tBatch:60 \tTrain Loss: 0.4439588189125061\n",
      "\n",
      "Epoch:12 \tBatch:80 \tTrain Loss: 0.4410504400730133\n",
      "\n",
      "Epoch:12 \tBatch:100 \tTrain Loss: 0.4436606764793396\n",
      "\n",
      "Epoch:12 \tBatch:120 \tTrain Loss: 0.45538732409477234\n",
      "\n",
      "Epoch:12 \tBatch:140 \tTrain Loss: 0.457234263420105\n",
      "\n",
      "Epoch:12 \tBatch:160 \tTrain Loss: 0.46511411666870117\n",
      "\n",
      "Epoch:12 \tBatch:180 \tTrain Loss: 0.4732568562030792\n",
      "\n",
      "Epoch:12 \tBatch:200 \tTrain Loss: 0.472220242023468\n",
      "\n",
      "Epoch: 12 \tTraining Loss: 0.472220 \tValidation Loss: 0.689296\n",
      "Epoch:13 \tBatch:20 \tTrain Loss: 0.5150457620620728\n",
      "\n",
      "Epoch:13 \tBatch:40 \tTrain Loss: 0.5195431709289551\n",
      "\n",
      "Epoch:13 \tBatch:60 \tTrain Loss: 0.519108235836029\n",
      "\n",
      "Epoch:13 \tBatch:80 \tTrain Loss: 0.5177779793739319\n",
      "\n",
      "Epoch:13 \tBatch:100 \tTrain Loss: 0.5098490715026855\n",
      "\n",
      "Epoch:13 \tBatch:120 \tTrain Loss: 0.5031396746635437\n",
      "\n",
      "Epoch:13 \tBatch:140 \tTrain Loss: 0.49675047397613525\n",
      "\n",
      "Epoch:13 \tBatch:160 \tTrain Loss: 0.4878876507282257\n",
      "\n",
      "Epoch:13 \tBatch:180 \tTrain Loss: 0.48632508516311646\n",
      "\n",
      "Epoch:13 \tBatch:200 \tTrain Loss: 0.48780542612075806\n",
      "\n",
      "Epoch: 13 \tTraining Loss: 0.487805 \tValidation Loss: 0.679212\n",
      "Validation loss decreased (0.687650 --> 0.679212). Saving model...\n",
      "Epoch:14 \tBatch:20 \tTrain Loss: 0.4255324900150299\n",
      "\n",
      "Epoch:14 \tBatch:40 \tTrain Loss: 0.5114131569862366\n",
      "\n",
      "Epoch:14 \tBatch:60 \tTrain Loss: 0.46419456601142883\n",
      "\n",
      "Epoch:14 \tBatch:80 \tTrain Loss: 0.4430348873138428\n",
      "\n",
      "Epoch:14 \tBatch:100 \tTrain Loss: 0.4503583014011383\n",
      "\n",
      "Epoch:14 \tBatch:120 \tTrain Loss: 0.46120044589042664\n",
      "\n",
      "Epoch:14 \tBatch:140 \tTrain Loss: 0.4692176878452301\n",
      "\n",
      "Epoch:14 \tBatch:160 \tTrain Loss: 0.4688112437725067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:14 \tBatch:180 \tTrain Loss: 0.46300357580184937\n",
      "\n",
      "Epoch:14 \tBatch:200 \tTrain Loss: 0.45773959159851074\n",
      "\n",
      "Epoch: 14 \tTraining Loss: 0.457740 \tValidation Loss: 0.701392\n",
      "Epoch:15 \tBatch:20 \tTrain Loss: 0.4917726516723633\n",
      "\n",
      "Epoch:15 \tBatch:40 \tTrain Loss: 0.4135316014289856\n",
      "\n",
      "Epoch:15 \tBatch:60 \tTrain Loss: 0.432269424200058\n",
      "\n",
      "Epoch:15 \tBatch:80 \tTrain Loss: 0.4558653235435486\n",
      "\n",
      "Epoch:15 \tBatch:100 \tTrain Loss: 0.44621768593788147\n",
      "\n",
      "Epoch:15 \tBatch:120 \tTrain Loss: 0.44902488589286804\n",
      "\n",
      "Epoch:15 \tBatch:140 \tTrain Loss: 0.44585901498794556\n",
      "\n",
      "Epoch:15 \tBatch:160 \tTrain Loss: 0.4369741678237915\n",
      "\n",
      "Epoch:15 \tBatch:180 \tTrain Loss: 0.44436949491500854\n",
      "\n",
      "Epoch:15 \tBatch:200 \tTrain Loss: 0.44636186957359314\n",
      "\n",
      "Epoch: 15 \tTraining Loss: 0.446362 \tValidation Loss: 0.716523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            # clear gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forwarward pass\n",
    "            output = model(data)\n",
    "            \n",
    "            # calculate batch loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # perform optimization step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update training loss\n",
    "            train_loss += ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            if (batch_idx + 1) % 20 == 0:\n",
    "                print(f'Epoch:{epoch} \\tBatch:{batch_idx + 1} \\tTrain Loss: {train_loss}\\n')\n",
    "                \n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss += ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "           \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        # save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# define loaders_transfer\n",
    "loaders_transfer = {'train': train_loader,\n",
    "                   'valid': valid_loader,\n",
    "                   'test': test_loader}\n",
    "\n",
    "model_transfer = train(15, loaders_transfer, model_transfer, optimizer, criterion, use_cuda, 'skin-cancer-model_transfer.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_transfer.load_state_dict(torch.load('skin-cancer-model_transfer.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.784682\n",
      "\n",
      "\n",
      "Test Accuracy: 67% (403/600)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)       \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        output = F.softmax(output, dim=1)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (100. * correct / total, correct, total))\n",
    "\n",
    "test(loaders_transfer, model_transfer, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
